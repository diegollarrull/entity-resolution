{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Entity Resolution\n",
    "* Xavier Ignacio Gonzalez (xig2000@columbia.edu)\n",
    "* Woojin Kim (wk2246@columbia.edu)\n",
    "* Diego Miguel Llarrull (dml2189@columbia.edu)\n",
    "\n",
    "## Techniques \n",
    "The approach that we designed in order to resolve identical entities is a three-level matching approach. \n",
    "1. We matched entries sharing equally normalized phone numbers. \n",
    "2. We matched the remaining unmatched entries by mathcing normalised venue names.\n",
    "3. We matched as many venues as possible within the remaining unmatched entries by using a cross-validated AdaBoost classifier by analysing pairwise similarity ratings on each venue feature.\n",
    "\n",
    "### Processing\n",
    "As the description suggests, prior to all analysis, the dataset was preprocessed by normalizing its features in order to make them comparable. This involved\n",
    "* removing conflicting Unicode characters,punctuaction marks\n",
    "* shifting all characters to lowercase\n",
    "* preserving only the domain names for the URLs\n",
    "* expanding often-used acronyms,\n",
    "* and formalizing null values by shifting them to Python's `None`.\n",
    "\n",
    "Once preprocessed, the dataset was extended by using the `street-address` Python address normalization library. This library basically parses addresses and normalized their structure into five fields: *house*, *street_name*, *street_type*, *suite_num* and *suite_type*. Rather than dropping the original field, we kept it and added the aforementioned columns to the datasets.\n",
    "\n",
    "In order to avoid all pairwise comparisons, we used phone numbers to screen easily matched entities. On the training set, just using phone numbers resulted in over 99% recall, finding 225 out of 360 entries in the match file. Then we checked if entities have unique perfect name matching to find more easily matched entities. Uniquely matching enables to avoid matching franchises and repeated names that might match otherwise. Added on top of the phone number matching, we find about 333 of the 360 matches, still keeping over 99% recall. Consequently, phone numbers and the unique names were the most important features powering our technique.\n",
    "\n",
    "### Machine learning classification\n",
    "After this, we used machine learning to classify the remainder of the entries. We created a pairwise comparison row for every entry in the Locu and Foursquare datasets. We then calculated the distance given the latitude/longitude coordinates, word similarity for majority of the fields, as well as the longest common subsequence for the name pairs.\n",
    "\n",
    "We tried a variety of classification techniques, including random forest, extra trees, bagging, AdaBoost, and decision trees; which all yielded similar results. After repeated cross validation to search the best hyperparameters, we used an AdaBoost classifier to submit our final results.\n",
    "\n",
    "### Results\n",
    "Our submission for the test set resulted in the following scores on the Instabase leaderboard:\n",
    "* Recall: 99.57%\n",
    "* Precision: 96.67%\n",
    "* F1 score: 98.10%\n",
    "\n",
    "\n",
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import streetaddress as sa\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "PATH = \"Prakhar/er-assignment/fs/Instabase%20Drive/files/datasets/\"\n",
    "FILES = {\n",
    "    \"foursquare_test\": \"foursquare_test_hard.json\",\n",
    "    \"locu_test\": \"locu_test_hard.json\",\n",
    "    \"matches\": \"matches_train_hard.csv\",\n",
    "    \"foursquare_train\": \"foursquare_train_hard.json\",\n",
    "    \"locu_train\": \"locu_train_hard.json\"\n",
    "}\n",
    "\n",
    "#Instabase load\n",
    "# fs_train = pd.read_json(ib.open(PATH + FILES[\"foursquare_train\"]))\n",
    "# fs_test = pd.read_json(ib.open(PATH + FILES[\"foursquare_test\"]))\n",
    "# lc_train = pd.read_json(ib.open(PATH + FILES[\"locu_train\"]))\n",
    "# lc_test = pd.read_json(ib.open(PATH + FILES[\"locu_test\"]))\n",
    "# matches = pd.read_csv(ib.open(PATH + FILES[\"matches\"]))\n",
    "\n",
    "#Local load\n",
    "fs_train = pd.read_json('data/foursquare_train_hard.json')\n",
    "fs_test = pd.read_json('data/foursquare_test_hard.json')\n",
    "lc_train = pd.read_json('data/locu_train_hard.json')\n",
    "lc_test = pd.read_json('data/locu_test_hard.json')\n",
    "matches = pd.read_csv('data/matches_train_hard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_distance(pt1, pt2):\n",
    "    return math.sqrt( (pt1[0] - pt2[0])**2 + (pt1[1] - pt2[1])**2 )\n",
    "    \n",
    "def string_similarity(str1, str2):\n",
    "    return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "def calc_lcs(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in xrange(1 + len(s1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in xrange(1, 1 + len(s1)):\n",
    "        for y in xrange(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "                \n",
    "    return len(s1[x_longest - longest: x_longest])\n",
    "\n",
    "# Normalizes street addresses using the streetaddress library. All normalized fields are added as columns\n",
    "def addr_parse(address):\n",
    "    if address is not None: \n",
    "        addr_parser = sa.StreetAddressParser()\n",
    "        addr = addr_parser.parse(address)\n",
    "        format = {'house': [addr['house']],\n",
    "                  'street_name': [addr['street_name']],\n",
    "                  'street_type': [addr['street_type']],\n",
    "                  'suite_num': [addr['suite_num']],\n",
    "                  'suite_type': [addr['suite_type']] }   \n",
    "    else: \n",
    "        format = {'house': [None],\n",
    "                  'street_name': [None],\n",
    "                  'street_type': [None],\n",
    "                  'suite_num': [None],\n",
    "                  'suite_type': [None] }\n",
    "    rv = pd.DataFrame(data = format)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "data_list = {'fs_train': fs_train,\n",
    "             'fs_test': fs_test,\n",
    "             'lc_train': lc_train,\n",
    "             'lc_test': lc_test }\n",
    "\n",
    "fs_train_phone_dir, fs_test_phone_dir = {}, {}\n",
    "lc_train_phone_dir, lc_test_phone_dir = {}, {}\n",
    "phone_dir = {'fs_train': fs_train_phone_dir,\n",
    "                   'fs_test': fs_test_phone_dir,\n",
    "                   'lc_train': lc_train_phone_dir,\n",
    "                   'lc_test': lc_test_phone_dir }\n",
    "\n",
    "for df_name, df in data_list.iteritems():\n",
    "    df.drop(['country', 'region', 'locality'], inplace=True, axis=1)\n",
    "    \n",
    "    df.replace([''], [None], inplace=True)\n",
    "    \n",
    "    df['id'] = df['id'].astype('str')\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'])\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'])\n",
    "    \n",
    "    # Unicode chars to replace\n",
    "    df['name'].replace([u\"\\xe9\"], ['e'], regex=True, inplace=True)\n",
    "    df['name'].replace([u\"\\xed\"], ['i'], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\u2019'], [''], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\xc7'], ['c'], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\u2013'], ['-'], regex=True, inplace=True)\n",
    "    \n",
    "    df['name'].replace([r':|\\'|,|\\.|-'], [''], regex=True, inplace=True)\n",
    "    df['name'].replace(['&'], ['and'], regex=True, inplace=True)\n",
    "    df['name'].replace(['\\s+|\\/'], [' '], regex=True, inplace=True)\n",
    "    df['name'].replace(['\\s'], [''], regex=True, inplace=True)\n",
    "\n",
    "    df['name'] = df['name'].astype(str).str.lower()\n",
    "    \n",
    "    df['phone'].replace([r'\\(|\\)|\\s|-'], [''], regex=True, inplace=True)\n",
    "    \n",
    "    # Make a phone directory\n",
    "    current_phone_dir = phone_dir[df_name]\n",
    "    for i, row in df.iterrows():\n",
    "        if row['phone'] != None:\n",
    "            current_phone_dir[row['phone']] = row['id']\n",
    "            \n",
    "    df['street_address'].replace([r'<sup>|<\\/sup>'], [''], regex=True, inplace=True)\n",
    "    df['street_address'].replace([r'\\.'], [''], regex=True, inplace=True)\n",
    "    df['street_address'].replace([r'Jfk'], ['John F Kennedy'], regex=True, inplace=True)\n",
    "    df['street_address'] = df['street_address'].astype(str)\n",
    "    \n",
    "    df['website'].replace([u\"\\u200e\"], [''], regex=True, inplace=True)\n",
    "    df['website'].replace([r'http(s)?://(www.)?|\\\\u200e'], [''], regex=True, inplace=True)\n",
    "    df['website'].replace([r'\\..*'], [''], regex=True, inplace=True)\n",
    "    df['website'] = df['website'].astype(str).str.lower()\n",
    "    df['website'].replace(['None'], [None], inplace=True)\n",
    "    \n",
    "    \n",
    "c = fs_train['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "fs_train = pd.concat([fs_train,cols], axis = 1)\n",
    "\n",
    "c = fs_test['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "fs_test = pd.concat([fs_test,cols], axis = 1)\n",
    "\n",
    "c = lc_train['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "lc_train = pd.concat([lc_train,cols], axis = 1)\n",
    "\n",
    "c = lc_test['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "lc_test = pd.concat([lc_test,cols], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phone number matching\n",
    "Entities with matching phone numbers are always matching, so we process these first and reduce the size of the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_train = {}\n",
    "for lc_phone in lc_train_phone_dir:\n",
    "    if lc_phone in fs_train_phone_dir:\n",
    "        lc_id = lc_train_phone_dir[lc_phone]\n",
    "        fs_id = fs_train_phone_dir[lc_phone]\n",
    "        matches_train[lc_id] = fs_id\n",
    "\n",
    "matches_test = {}\n",
    "for lc_phone in lc_test_phone_dir:\n",
    "    if lc_phone in fs_test_phone_dir:\n",
    "        lc_id = lc_test_phone_dir[lc_phone]\n",
    "        fs_id = fs_test_phone_dir[lc_phone]\n",
    "        matches_test[lc_id] = fs_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we confirm that the accuracy is nearly 100% for the training set using only phone number matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991189427313\n"
     ]
    }
   ],
   "source": [
    "how_true = []\n",
    "for lc_id, fs_id in matches_train.iteritems():\n",
    "    fs_match_id = matches[matches['locu_id'] == lc_id]['foursquare_id']\n",
    "    if len(fs_match_id > 0):\n",
    "        how_true.append(fs_match_id.iloc[0] == fs_id)\n",
    "print(sum(how_true) / float(len(matches_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data sets with phone matches removed\n",
    "lc_train_not_matching = [not x for x in lc_train['id'].isin(matches_train.keys())]\n",
    "fs_train_not_matching = [not x for x in fs_train['id'].isin(matches_train.values())]\n",
    "lc_test_not_matching = [not x for x in lc_test['id'].isin(matches_test.keys())]\n",
    "fs_test_not_matching = [not x for x in fs_test['id'].isin(matches_test.values())]\n",
    "\n",
    "lc_train = lc_train[lc_train_not_matching]\n",
    "fs_train = fs_train[fs_train_not_matching]\n",
    "lc_test = lc_test[lc_test_not_matching]\n",
    "fs_test = fs_test[fs_test_not_matching]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name matching\n",
    "Entities with matching names are always matching, so we also process these in advance to further reduce the size of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "data_list = {'fs_train': fs_train,\n",
    "             'fs_test': fs_test,\n",
    "             'lc_train': lc_train,\n",
    "             'lc_test': lc_test }\n",
    "\n",
    "fs_train_name_dir, fs_test_name_dir = {}, {}\n",
    "lc_train_name_dir, lc_test_name_dir = {}, {}\n",
    "name_dir = {'fs_train': fs_train_name_dir,\n",
    "            'fs_test': fs_test_name_dir,\n",
    "            'lc_train': lc_train_name_dir,\n",
    "            'lc_test': lc_test_name_dir }\n",
    "\n",
    "for df_name, df in data_list.iteritems():\n",
    "    # Make a name directory\n",
    "    current_name_dir = name_dir[df_name]\n",
    "    for i, row in df.iterrows():\n",
    "        if row['name'] != None:\n",
    "            if row['name'] in current_name_dir.keys(): \n",
    "                current_name_dir[row['name']] = current_name_dir[row['name']] + [row['id']]\n",
    "            else:\n",
    "                current_name_dir[row['name']] = [row['id']]\n",
    "\n",
    "\n",
    "added = 0\n",
    "for lc_name in lc_train_name_dir.keys():\n",
    "    if lc_name in fs_train_name_dir.keys():\n",
    "        lc_ids = lc_train_name_dir[lc_name]\n",
    "        fs_ids = fs_train_name_dir[lc_name]\n",
    "        if (len(lc_ids) == 1) and (len(fs_ids) == 1):\n",
    "            matches_train[lc_ids[0]] = fs_ids[0]\n",
    "            added = added + 1\n",
    "\n",
    "print len(matches_train)\n",
    "            \n",
    "added = 0            \n",
    "for lc_name in lc_test_name_dir.keys():\n",
    "    if lc_name in fs_test_name_dir.keys():\n",
    "        lc_ids = lc_test_name_dir[lc_name]\n",
    "        fs_ids = fs_test_name_dir[lc_name]\n",
    "        if (len(lc_ids) == 1) and (len(fs_ids) == 1):\n",
    "            matches_test[lc_ids[0]] = fs_ids[0]\n",
    "            added = added + 1\n",
    "            \n",
    "print len(matches_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we confirm that the accuracy is 100% of the training set using phone number matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993993993994\n"
     ]
    }
   ],
   "source": [
    "how_true = []\n",
    "for lc_id, fs_id in matches_train.iteritems():\n",
    "    fs_match_id = matches[matches['locu_id'] == lc_id]['foursquare_id']\n",
    "    if len(fs_match_id > 0):\n",
    "        how_true.append(fs_match_id.iloc[0] == fs_id)\n",
    "print(sum(how_true) / float(len(matches_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data sets with phone matches removed\n",
    "lc_train_not_matching = [not x for x in lc_train['id'].isin(matches_train.keys())]\n",
    "fs_train_not_matching = [not x for x in fs_train['id'].isin(matches_train.values())]\n",
    "lc_test_not_matching = [not x for x in lc_test['id'].isin(matches_test.keys())]\n",
    "fs_test_not_matching = [not x for x in fs_test['id'].isin(matches_test.values())]\n",
    "\n",
    "lc_train = lc_train[lc_train_not_matching]\n",
    "fs_train = fs_train[fs_train_not_matching]\n",
    "lc_test = lc_test[lc_test_not_matching]\n",
    "fs_test = fs_test[fs_test_not_matching]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Row Combinations for Machine Learning\n",
    "Append a prefix to identify the columns when concatenated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in [fs_train, fs_test]:\n",
    "    df.columns = ['fs_' + str(i) for i in list(df.columns)]\n",
    "for df in [lc_train, lc_test]:\n",
    "    df.columns = ['lc_' + str(i) for i in list(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LC data is repeated row at a time, then FS data is repeated entirely at a time. The two are concatenated to create the combo data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_left =  lc_train.loc[np.repeat(lc_train.index.values, len(lc_train))].reset_index(drop=True)\n",
    "train_right =  pd.concat([fs_train]*len(fs_train), ignore_index=True)\n",
    "train = pd.concat([train_left, train_right], axis=1)\n",
    "\n",
    "test_left =  lc_test.loc[np.repeat(lc_test.index.values, len(lc_test))].reset_index(drop=True)\n",
    "test_right =  pd.concat([fs_test]*len(fs_test), ignore_index=True)\n",
    "test = pd.concat([test_left, test_right], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add match status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Match dictionary\n",
    "match_dict = {}\n",
    "for i, row in matches.iterrows():\n",
    "    match_dict[row['locu_id']] = row['foursquare_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_column = []\n",
    "for i, row in train.iterrows():\n",
    "    lc_id = row['lc_id']\n",
    "    fs_id = row['fs_id']\n",
    "    if (lc_id in match_dict) and (match_dict[lc_id] == fs_id):\n",
    "        match_column.append(1)\n",
    "    else:\n",
    "        match_column.append(0)\n",
    "match_column = np.array(match_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate various distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "Starting iteration #0\n",
      "Processing distances...\n",
      "Processing names...\n",
      "Processing ZIP codes...\n",
      "Processing phone numbers...\n",
      "Processing URLs...\n",
      "Processing street addresses...\n",
      "Processing LCS...\n",
      "#####\n",
      "Starting iteration #1\n",
      "Processing distances...\n",
      "Processing names...\n",
      "Processing ZIP codes...\n",
      "Processing phone numbers...\n",
      "Processing URLs...\n",
      "Processing street addresses...\n",
      "Processing LCS...\n",
      "#####\n",
      "Processing Finished!\n"
     ]
    }
   ],
   "source": [
    "data_list = [train, test]\n",
    "\n",
    "for d_i, df in enumerate(data_list):\n",
    "    print(\"#####\\nStarting iteration #{}\".format(d_i))\n",
    "    \n",
    "    print('Processing distances...')\n",
    "    distance = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_loc = (row['lc_latitude'], row['lc_longitude'])\n",
    "        fs_loc = (row['fs_latitude'], row['fs_longitude'])\n",
    "        distance.append(find_distance(lc_loc, fs_loc))\n",
    "\n",
    "    print('Processing names...')\n",
    "    name_dist = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_name = row['lc_name']\n",
    "        fs_name = row['fs_name']\n",
    "        name_dist.append(string_similarity(lc_name, fs_name))\n",
    "\n",
    "    print('Processing ZIP codes...')\n",
    "    zip_dist = []\n",
    "    zip_missing = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_zip = row['lc_postal_code']\n",
    "        fs_zip = row['fs_postal_code']\n",
    "        if lc_zip and fs_zip:\n",
    "            zip_dist.append(string_similarity(lc_zip, fs_zip))\n",
    "            zip_missing.append(0)\n",
    "        else:\n",
    "            zip_dist.append(np.nan)\n",
    "            zip_missing.append(1)\n",
    "\n",
    "    print('Processing phone numbers...')\n",
    "    phone_dist = []\n",
    "    phone_missing = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_phone = row['lc_phone']\n",
    "        fs_phone = row['fs_phone']\n",
    "        if lc_phone and fs_phone:\n",
    "            phone_dist.append(string_similarity(lc_phone, fs_phone))\n",
    "            phone_missing.append(0)\n",
    "        else:\n",
    "            phone_dist.append(np.nan)\n",
    "            phone_missing.append(1)\n",
    "\n",
    "    print('Processing URLs...')\n",
    "    url_dist = []\n",
    "    url_missing = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_url = row['lc_website']\n",
    "        fs_url = row['fs_website']\n",
    "        if lc_url and fs_url:\n",
    "            url_dist.append(string_similarity(lc_url, fs_url))\n",
    "            url_missing.append(0)\n",
    "        else:\n",
    "            url_dist.append(np.nan)\n",
    "            url_missing.append(1)\n",
    "            \n",
    "    print('Processing street addresses...')\n",
    "    house_sim, house_missing = [], []\n",
    "    street_name_sim, street_name_missing = [], []\n",
    "    street_type_sim, street_type_missing = [], []\n",
    "    suite_num_sim, suite_num_missing = [], []\n",
    "    suite_type_sim, suite_type_missing = [], []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_house = row['lc_house']\n",
    "        fs_house = row['fs_house']\n",
    "        \n",
    "        lc_street_name = row['lc_street_name']\n",
    "        fs_street_name = row['fs_street_name']\n",
    "        \n",
    "        lc_street_type = row['lc_street_type']\n",
    "        fs_street_type = row['fs_street_type']\n",
    "        \n",
    "        lc_suite_num = row['lc_suite_num']\n",
    "        fs_suite_num = row['fs_suite_num']\n",
    "        \n",
    "        lc_suite_type = row['lc_suite_type']\n",
    "        fs_suite_type = row['fs_suite_type']\n",
    "        \n",
    "        if lc_house and fs_house:\n",
    "            house_sim.append(string_similarity(lc_house, fs_house))\n",
    "            house_missing.append(0)\n",
    "        else:\n",
    "            house_sim.append(np.nan)\n",
    "            house_missing.append(1)\n",
    "        \n",
    "        if lc_street_name and fs_street_name:\n",
    "            street_name_sim.append(string_similarity(lc_street_name, fs_street_name))\n",
    "            street_name_missing.append(0)\n",
    "        else:\n",
    "            street_name_sim.append(np.nan)\n",
    "            street_name_missing.append(1)\n",
    "            \n",
    "        if lc_street_type and fs_street_type:\n",
    "            street_type_sim.append(string_similarity(lc_street_type, fs_street_type))\n",
    "            street_type_missing.append(0)\n",
    "        else:\n",
    "            street_type_sim.append(np.nan)\n",
    "            street_type_missing.append(1)\n",
    "        \n",
    "        if lc_suite_num and fs_suite_num:\n",
    "            suite_num_sim.append(string_similarity(lc_suite_num, fs_suite_num))\n",
    "            suite_num_missing.append(0)\n",
    "        else:\n",
    "            suite_num_sim.append(np.nan)\n",
    "            suite_num_missing.append(1)\n",
    "        \n",
    "        if lc_suite_type and fs_suite_type:\n",
    "            suite_type_sim.append(string_similarity(lc_suite_type, fs_suite_type))\n",
    "            suite_type_missing.append(0)\n",
    "        else:\n",
    "            suite_type_sim.append(np.nan)\n",
    "            suite_type_missing.append(1)\n",
    "            \n",
    "    print('Processing LCS...')\n",
    "    lcs = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_name = row['lc_name']\n",
    "        fs_name = row['fs_name']\n",
    "        lcs.append(calc_lcs(lc_name, fs_name))\n",
    "    \n",
    "    d = {'distance': distance,\n",
    "         'name_sim': name_dist,\n",
    "         'zip_sim': zip_dist,  'zip_missing': zip_missing,\n",
    "         'phone_sim': phone_dist, 'phone_missing': phone_missing,\n",
    "         'url_sim': url_dist, 'url_missing': url_missing,\n",
    "         'house_sim': house_sim, 'house_missing': house_missing,\n",
    "         'street_name_sim': street_name_sim, 'street_name_missing': street_name_missing,\n",
    "         'street_type_sim': street_type_sim, 'street_type_missing': street_type_missing,\n",
    "         'suite_num_sim': suite_num_sim, 'suite_num_missing': suite_num_missing,\n",
    "         'suite_type_sim': suite_type_sim, 'suite_type_missing': suite_type_missing,\n",
    "         'lcs': lcs }\n",
    "    \n",
    "    if d_i == 0:\n",
    "        train_data = pd.DataFrame(d).fillna(0)\n",
    "    else:\n",
    "        test_data = pd.DataFrame(d).fillna(0)\n",
    "\n",
    "print(\"#####\\nProcessing Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values (replace with mean value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# ip = Imputer(missing_values = 'NaN')\n",
    "# ip.fit(pd.concat([train_data, test_data], axis=0))\n",
    "\n",
    "# train_data = pd.DataFrame(ip.fit_transform(train_data))\n",
    "# test_data = pd.DataFrame(ip.fit_transform(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* To-do: Hyperparameter Tuning\n",
    "\n",
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "def cv_run_ada(train_data, train_labels, test_data, test_labels):\n",
    "    model = AdaBoostClassifier(random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_bag(train_data, train_labels, test_data, test_labels):\n",
    "    model = BaggingClassifier(max_features=1.0, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_et(train_data, train_labels, test_data, test_labels):\n",
    "    model = ExtraTreesClassifier(n_estimators=100, max_features=None, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_rf(train_data, train_labels, test_data, test_labels):\n",
    "    model = RandomForestClassifier(n_estimators=100, max_features=None, n_jobs=-1, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_dt(train_data, train_labels, test_data, test_labels):\n",
    "    model = DecisionTreeClassifier(max_features=None, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "#def cv_run_xg(train_data, train_labels, test_data, test_labels):\n",
    "#    model = XGBClassifier().fit(train_data, train_labels)\n",
    "#    return model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "1.0\n",
      "1.0\n",
      "Overall Recall: 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(match_column, n_folds=5, random_state=1, shuffle=True)\n",
    "\n",
    "overall_corr = 0\n",
    "wrong_indices = []\n",
    "for train_index, test_index in skf:\n",
    "    cv_train_data = train_data.loc[train_index]\n",
    "    cv_train_labels = match_column[train_index]\n",
    "    cv_test_data = train_data.loc[test_index]\n",
    "    cv_test_labels = match_column[test_index]\n",
    "    \n",
    "    preds = cv_run_rf(cv_train_data, cv_train_labels, cv_test_data, cv_test_labels)\n",
    "\n",
    "    fold_corr = sum(preds[cv_test_labels == 1])\n",
    "    overall_corr += fold_corr\n",
    "    \n",
    "    # Collect wrong indices to check\n",
    "    wrong_ix = [not x for x in preds[cv_test_labels == 1]]\n",
    "    wrong_indices += list(cv_test_data[cv_test_labels == 1][wrong_ix].index)\n",
    "        \n",
    "    fold_acc = fold_corr / float(sum(cv_test_labels))\n",
    "    print(fold_acc)\n",
    "    \n",
    "print(\"Overall Recall: {}\".format(float(overall_corr) / sum(match_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] n_estimators=33, learning_rate=0.473577366272, random_state=1 ...\n",
      "[CV]  n_estimators=33, learning_rate=0.473577366272, random_state=1, score=0.999719 -   1.3s\n",
      "[CV] n_estimators=33, learning_rate=0.473577366272, random_state=1 ...\n",
      "[CV]  n_estimators=33, learning_rate=0.473577366272, random_state=1, score=0.999719 -   1.3s\n",
      "[CV] n_estimators=33, learning_rate=0.473577366272, random_state=1 ...\n",
      "[CV]  n_estimators=33, learning_rate=0.473577366272, random_state=1, score=0.999790 -   1.3s\n",
      "[CV] n_estimators=33, learning_rate=0.473577366272, random_state=1 ...\n",
      "[CV]  n_estimators=33, learning_rate=0.473577366272, random_state=1, score=0.999860 -   1.3s\n",
      "[CV] n_estimators=33, learning_rate=0.473577366272, random_state=1 ...\n",
      "[CV]  n_estimators=33, learning_rate=0.473577366272, random_state=1, score=0.999860 -   1.3s\n",
      "[CV] n_estimators=163, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=163, learning_rate=0.893313110297, random_state=1, score=0.999299 -   6.4s\n",
      "[CV] n_estimators=163, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=163, learning_rate=0.893313110297, random_state=1, score=0.999930 -   6.4s\n",
      "[CV] n_estimators=163, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=163, learning_rate=0.893313110297, random_state=1, score=0.999790 -   6.3s\n",
      "[CV] n_estimators=163, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=163, learning_rate=0.893313110297, random_state=1, score=0.999790 -   6.4s\n",
      "[CV] n_estimators=163, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=163, learning_rate=0.893313110297, random_state=1, score=0.999860 -   6.5s\n",
      "[CV] n_estimators=18, learning_rate=0.104320838398, random_state=1 ...\n",
      "[CV]  n_estimators=18, learning_rate=0.104320838398, random_state=1, score=0.999579 -   0.7s\n",
      "[CV] n_estimators=18, learning_rate=0.104320838398, random_state=1 ...\n",
      "[CV]  n_estimators=18, learning_rate=0.104320838398, random_state=1, score=0.999579 -   0.7s\n",
      "[CV] n_estimators=18, learning_rate=0.104320838398, random_state=1 ...\n",
      "[CV]  n_estimators=18, learning_rate=0.104320838398, random_state=1, score=0.999649 -   0.7s\n",
      "[CV] n_estimators=18, learning_rate=0.104320838398, random_state=1 ...\n",
      "[CV]  n_estimators=18, learning_rate=0.104320838398, random_state=1, score=0.999790 -   0.7s\n",
      "[CV] n_estimators=18, learning_rate=0.104320838398, random_state=1 ...\n",
      "[CV]  n_estimators=18, learning_rate=0.104320838398, random_state=1, score=0.999649 -   0.7s\n",
      "[CV] n_estimators=99, learning_rate=0.942099468936, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.942099468936, random_state=1, score=0.999229 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.942099468936, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.942099468936, random_state=1, score=0.999930 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.942099468936, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.942099468936, random_state=1, score=0.999860 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.942099468936, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.942099468936, random_state=1, score=0.999790 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.942099468936, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.942099468936, random_state=1, score=0.999860 -   3.9s\n",
      "[CV] n_estimators=21, learning_rate=0.345730171272, random_state=1 ...\n",
      "[CV]  n_estimators=21, learning_rate=0.345730171272, random_state=1, score=0.999790 -   0.8s\n",
      "[CV] n_estimators=21, learning_rate=0.345730171272, random_state=1 ...\n",
      "[CV]  n_estimators=21, learning_rate=0.345730171272, random_state=1, score=0.999579 -   0.8s\n",
      "[CV] n_estimators=21, learning_rate=0.345730171272, random_state=1 ...\n",
      "[CV]  n_estimators=21, learning_rate=0.345730171272, random_state=1, score=0.999649 -   0.8s\n",
      "[CV] n_estimators=21, learning_rate=0.345730171272, random_state=1 ...\n",
      "[CV]  n_estimators=21, learning_rate=0.345730171272, random_state=1, score=0.999860 -   0.9s\n",
      "[CV] n_estimators=21, learning_rate=0.345730171272, random_state=1 ...\n",
      "[CV]  n_estimators=21, learning_rate=0.345730171272, random_state=1, score=0.999860 -   0.9s\n",
      "[CV] n_estimators=44, learning_rate=0.590714268185, random_state=1 ...\n",
      "[CV]  n_estimators=44, learning_rate=0.590714268185, random_state=1, score=0.999719 -   1.7s\n",
      "[CV] n_estimators=44, learning_rate=0.590714268185, random_state=1 ...\n",
      "[CV]  n_estimators=44, learning_rate=0.590714268185, random_state=1, score=0.999719 -   1.7s\n",
      "[CV] n_estimators=44, learning_rate=0.590714268185, random_state=1 ...\n",
      "[CV]  n_estimators=44, learning_rate=0.590714268185, random_state=1, score=0.999790 -   1.7s\n",
      "[CV] n_estimators=44, learning_rate=0.590714268185, random_state=1 ...\n",
      "[CV]  n_estimators=44, learning_rate=0.590714268185, random_state=1, score=0.999860 -   1.7s\n",
      "[CV] n_estimators=44, learning_rate=0.590714268185, random_state=1 ...\n",
      "[CV]  n_estimators=44, learning_rate=0.590714268185, random_state=1, score=0.999860 -   1.8s\n",
      "[CV] n_estimators=150, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=150, learning_rate=0.467080606428, random_state=1, score=0.999649 -   5.9s\n",
      "[CV] n_estimators=150, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=150, learning_rate=0.467080606428, random_state=1, score=0.999860 -   5.9s\n",
      "[CV] n_estimators=150, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=150, learning_rate=0.467080606428, random_state=1, score=0.999790 -   5.8s\n",
      "[CV] n_estimators=150, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=150, learning_rate=0.467080606428, random_state=1, score=0.999860 -   6.1s\n",
      "[CV] n_estimators=150, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=150, learning_rate=0.467080606428, random_state=1, score=0.999860 -   6.0s\n",
      "[CV] n_estimators=189, learning_rate=0.942099468936, random_state=1 ..\n",
      "[CV]  n_estimators=189, learning_rate=0.942099468936, random_state=1, score=0.999299 -   7.7s\n",
      "[CV] n_estimators=189, learning_rate=0.942099468936, random_state=1 ..\n",
      "[CV]  n_estimators=189, learning_rate=0.942099468936, random_state=1, score=0.999930 -   8.1s\n",
      "[CV] n_estimators=189, learning_rate=0.942099468936, random_state=1 ..\n",
      "[CV]  n_estimators=189, learning_rate=0.942099468936, random_state=1, score=0.999790 -   7.4s\n",
      "[CV] n_estimators=189, learning_rate=0.942099468936, random_state=1 ..\n",
      "[CV]  n_estimators=189, learning_rate=0.942099468936, random_state=1, score=0.999860 -   7.4s\n",
      "[CV] n_estimators=189, learning_rate=0.942099468936, random_state=1 ..\n",
      "[CV]  n_estimators=189, learning_rate=0.942099468936, random_state=1, score=0.999790 -   8.3s\n",
      "[CV] n_estimators=48, learning_rate=0.141205288094, random_state=1 ...\n",
      "[CV]  n_estimators=48, learning_rate=0.141205288094, random_state=1, score=0.999790 -   2.7s\n",
      "[CV] n_estimators=48, learning_rate=0.141205288094, random_state=1 ...\n",
      "[CV]  n_estimators=48, learning_rate=0.141205288094, random_state=1, score=0.999719 -   2.6s\n",
      "[CV] n_estimators=48, learning_rate=0.141205288094, random_state=1 ...\n",
      "[CV]  n_estimators=48, learning_rate=0.141205288094, random_state=1, score=0.999790 -   2.4s\n",
      "[CV] n_estimators=48, learning_rate=0.141205288094, random_state=1 ...\n",
      "[CV]  n_estimators=48, learning_rate=0.141205288094, random_state=1, score=0.999860 -   2.1s\n",
      "[CV] n_estimators=48, learning_rate=0.141205288094, random_state=1 ...\n",
      "[CV]  n_estimators=48, learning_rate=0.141205288094, random_state=1, score=0.999860 -   1.9s\n",
      "[CV] n_estimators=155, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=155, learning_rate=0.859925102284, random_state=1, score=0.999229 -   6.1s\n",
      "[CV] n_estimators=155, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=155, learning_rate=0.859925102284, random_state=1, score=0.999930 -   6.2s\n",
      "[CV] n_estimators=155, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=155, learning_rate=0.859925102284, random_state=1, score=0.999790 -   7.0s\n",
      "[CV] n_estimators=155, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=155, learning_rate=0.859925102284, random_state=1, score=0.999860 -   6.2s\n",
      "[CV] n_estimators=155, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=155, learning_rate=0.859925102284, random_state=1, score=0.999860 -   6.2s\n",
      "[CV] n_estimators=170, learning_rate=0.473577366272, random_state=1 ..\n",
      "[CV]  n_estimators=170, learning_rate=0.473577366272, random_state=1, score=0.999649 -   6.7s\n",
      "[CV] n_estimators=170, learning_rate=0.473577366272, random_state=1 ..\n",
      "[CV]  n_estimators=170, learning_rate=0.473577366272, random_state=1, score=0.999860 -   6.7s\n",
      "[CV] n_estimators=170, learning_rate=0.473577366272, random_state=1 ..\n",
      "[CV]  n_estimators=170, learning_rate=0.473577366272, random_state=1, score=0.999790 -   6.6s\n",
      "[CV] n_estimators=170, learning_rate=0.473577366272, random_state=1 ..\n",
      "[CV]  n_estimators=170, learning_rate=0.473577366272, random_state=1, score=0.999860 -   6.6s\n",
      "[CV] n_estimators=170, learning_rate=0.473577366272, random_state=1 ..\n",
      "[CV]  n_estimators=170, learning_rate=0.473577366272, random_state=1, score=0.999860 -   6.7s\n",
      "[CV] n_estimators=119, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=119, learning_rate=0.893313110297, random_state=1, score=0.999299 -   4.7s\n",
      "[CV] n_estimators=119, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=119, learning_rate=0.893313110297, random_state=1, score=0.999930 -   4.7s\n",
      "[CV] n_estimators=119, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=119, learning_rate=0.893313110297, random_state=1, score=0.999790 -   4.7s\n",
      "[CV] n_estimators=119, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=119, learning_rate=0.893313110297, random_state=1, score=0.999860 -   4.6s\n",
      "[CV] n_estimators=119, learning_rate=0.893313110297, random_state=1 ..\n",
      "[CV]  n_estimators=119, learning_rate=0.893313110297, random_state=1, score=0.999860 -   4.7s\n",
      "[CV] n_estimators=181, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=181, learning_rate=0.467080606428, random_state=1, score=0.999649 -   7.1s\n",
      "[CV] n_estimators=181, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=181, learning_rate=0.467080606428, random_state=1, score=0.999860 -   7.2s\n",
      "[CV] n_estimators=181, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=181, learning_rate=0.467080606428, random_state=1, score=0.999790 -   7.1s\n",
      "[CV] n_estimators=181, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=181, learning_rate=0.467080606428, random_state=1, score=0.999860 -   7.0s\n",
      "[CV] n_estimators=181, learning_rate=0.467080606428, random_state=1 ..\n",
      "[CV]  n_estimators=181, learning_rate=0.467080606428, random_state=1, score=0.999860 -   7.1s\n",
      "[CV] n_estimators=99, learning_rate=0.680795351981, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.680795351981, random_state=1, score=0.999439 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.680795351981, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.680795351981, random_state=1, score=0.999860 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.680795351981, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.680795351981, random_state=1, score=0.999790 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.680795351981, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.680795351981, random_state=1, score=0.999860 -   3.9s\n",
      "[CV] n_estimators=99, learning_rate=0.680795351981, random_state=1 ...\n",
      "[CV]  n_estimators=99, learning_rate=0.680795351981, random_state=1, score=0.999860 -   3.9s\n",
      "[CV] n_estimators=31, learning_rate=0.467080606428, random_state=1 ...\n",
      "[CV]  n_estimators=31, learning_rate=0.467080606428, random_state=1, score=0.999790 -   1.2s\n",
      "[CV] n_estimators=31, learning_rate=0.467080606428, random_state=1 ...\n",
      "[CV]  n_estimators=31, learning_rate=0.467080606428, random_state=1, score=0.999719 -   1.2s\n",
      "[CV] n_estimators=31, learning_rate=0.467080606428, random_state=1 ...\n",
      "[CV]  n_estimators=31, learning_rate=0.467080606428, random_state=1, score=0.999790 -   1.2s\n",
      "[CV] n_estimators=31, learning_rate=0.467080606428, random_state=1 ...\n",
      "[CV]  n_estimators=31, learning_rate=0.467080606428, random_state=1, score=0.999860 -   1.2s\n",
      "[CV] n_estimators=31, learning_rate=0.467080606428, random_state=1 ...\n",
      "[CV]  n_estimators=31, learning_rate=0.467080606428, random_state=1, score=0.999860 -   1.2s\n",
      "[CV] n_estimators=151, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=151, learning_rate=0.859925102284, random_state=1, score=0.999299 -   5.9s\n",
      "[CV] n_estimators=151, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=151, learning_rate=0.859925102284, random_state=1, score=0.999930 -   6.0s\n",
      "[CV] n_estimators=151, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=151, learning_rate=0.859925102284, random_state=1, score=0.999790 -   5.9s\n",
      "[CV] n_estimators=151, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=151, learning_rate=0.859925102284, random_state=1, score=0.999860 -   5.9s\n",
      "[CV] n_estimators=151, learning_rate=0.859925102284, random_state=1 ..\n",
      "[CV]  n_estimators=151, learning_rate=0.859925102284, random_state=1, score=0.999860 -   5.9s\n",
      "[CV] n_estimators=92, learning_rate=0.59055214407, random_state=1 ....\n",
      "[CV]  n_estimators=92, learning_rate=0.59055214407, random_state=1, score=0.999439 -   3.6s\n",
      "[CV] n_estimators=92, learning_rate=0.59055214407, random_state=1 ....\n",
      "[CV]  n_estimators=92, learning_rate=0.59055214407, random_state=1, score=0.999860 -   3.6s\n",
      "[CV] n_estimators=92, learning_rate=0.59055214407, random_state=1 ....\n",
      "[CV]  n_estimators=92, learning_rate=0.59055214407, random_state=1, score=0.999790 -   3.6s\n",
      "[CV] n_estimators=92, learning_rate=0.59055214407, random_state=1 ....\n",
      "[CV]  n_estimators=92, learning_rate=0.59055214407, random_state=1, score=0.999860 -   3.6s\n",
      "[CV] n_estimators=92, learning_rate=0.59055214407, random_state=1 ....\n",
      "[CV]  n_estimators=92, learning_rate=0.59055214407, random_state=1, score=0.999860 -   3.6s\n",
      "[CV] n_estimators=126, learning_rate=0.957388554279, random_state=1 ..\n",
      "[CV]  n_estimators=126, learning_rate=0.957388554279, random_state=1, score=0.999158 -   4.9s\n",
      "[CV] n_estimators=126, learning_rate=0.957388554279, random_state=1 ..\n",
      "[CV]  n_estimators=126, learning_rate=0.957388554279, random_state=1, score=0.999930 -   4.9s\n",
      "[CV] n_estimators=126, learning_rate=0.957388554279, random_state=1 ..\n",
      "[CV]  n_estimators=126, learning_rate=0.957388554279, random_state=1, score=0.999860 -   4.9s\n",
      "[CV] n_estimators=126, learning_rate=0.957388554279, random_state=1 ..\n",
      "[CV]  n_estimators=126, learning_rate=0.957388554279, random_state=1, score=0.999860 -   4.9s\n",
      "[CV] n_estimators=126, learning_rate=0.957388554279, random_state=1 ..\n",
      "[CV]  n_estimators=126, learning_rate=0.957388554279, random_state=1, score=0.999860 -   5.0s\n",
      "[CV] n_estimators=125, learning_rate=0.590714268185, random_state=1 ..\n",
      "[CV]  n_estimators=125, learning_rate=0.590714268185, random_state=1, score=0.999439 -   4.9s\n",
      "[CV] n_estimators=125, learning_rate=0.590714268185, random_state=1 ..\n",
      "[CV]  n_estimators=125, learning_rate=0.590714268185, random_state=1, score=0.999860 -   4.9s\n",
      "[CV] n_estimators=125, learning_rate=0.590714268185, random_state=1 ..\n",
      "[CV]  n_estimators=125, learning_rate=0.590714268185, random_state=1, score=0.999790 -   4.9s\n",
      "[CV] n_estimators=125, learning_rate=0.590714268185, random_state=1 ..\n",
      "[CV]  n_estimators=125, learning_rate=0.590714268185, random_state=1, score=0.999860 -   4.9s\n",
      "[CV] n_estimators=125, learning_rate=0.590714268185, random_state=1 ..\n",
      "[CV]  n_estimators=125, learning_rate=0.590714268185, random_state=1, score=0.999860 -   4.9s\n",
      "[CV] n_estimators=10, learning_rate=0.973472631719, random_state=1 ...\n",
      "[CV]  n_estimators=10, learning_rate=0.973472631719, random_state=1, score=0.999439 -   0.4s\n",
      "[CV] n_estimators=10, learning_rate=0.973472631719, random_state=1 ...\n",
      "[CV]  n_estimators=10, learning_rate=0.973472631719, random_state=1, score=0.999439 -   0.4s\n",
      "[CV] n_estimators=10, learning_rate=0.973472631719, random_state=1 ...\n",
      "[CV]  n_estimators=10, learning_rate=0.973472631719, random_state=1, score=0.999719 -   0.4s\n",
      "[CV] n_estimators=10, learning_rate=0.973472631719, random_state=1 ...\n",
      "[CV]  n_estimators=10, learning_rate=0.973472631719, random_state=1, score=0.999860 -   0.4s\n",
      "[CV] n_estimators=10, learning_rate=0.973472631719, random_state=1 ...\n",
      "[CV]  n_estimators=10, learning_rate=0.973472631719, random_state=1, score=0.999860 -   0.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  31 tasks       | elapsed:  1.3min\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "import random\n",
    "\n",
    "params = {'n_estimators': sp_randint(10,200),\n",
    "         'learning_rate': [random.uniform(0.1, 1.0) for i in range(1,20)],\n",
    "         'random_state': [1]}\n",
    "\n",
    "classifier = AdaBoostClassifier()\n",
    "cv_classifier = RandomizedSearchCV(classifier, param_distributions=params, n_iter=20, cv=5, verbose=3)\n",
    "model = cv_classifier.fit(train_data, match_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10577</th>\n",
       "      <th>38693</th>\n",
       "      <th>50111</th>\n",
       "      <th>47997</th>\n",
       "      <th>54951</th>\n",
       "      <th>67663</th>\n",
       "      <th>11535</th>\n",
       "      <th>29727</th>\n",
       "      <th>42480</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>distance</th>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.086440</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.025080</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.062195</td>\n",
       "      <td>0.034435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_missing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_sim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lcs</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name_sim</th>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone_missing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone_sim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>street_name_missing</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>street_name_sim</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>street_type_missing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>street_type_sim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suite_num_missing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suite_num_sim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suite_type_missing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suite_type_sim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_missing</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_sim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_missing</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_sim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         10577     38693     50111     47997     54951  \\\n",
       "distance              0.006468  0.086440  0.066754  0.000234  0.004729   \n",
       "house_missing         1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "house_sim             0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "lcs                  17.000000  4.000000  3.000000  7.000000  8.000000   \n",
       "name_sim              0.878049  0.421053  0.375000  0.533333  0.500000   \n",
       "phone_missing         1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "phone_sim             0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "street_name_missing   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "street_name_sim       0.727273  0.666667  0.000000  0.000000  0.166667   \n",
       "street_type_missing   1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "street_type_sim       0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "suite_num_missing     1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "suite_num_sim         0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "suite_type_missing    1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "suite_type_sim        0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "url_missing           0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "url_sim               1.000000  0.250000  0.100000  0.266667  0.250000   \n",
       "zip_missing           0.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "zip_sim               1.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                        67663     11535     29727     42480  \n",
       "distance             0.025080  0.008889  0.062195  0.034435  \n",
       "house_missing        1.000000  1.000000  1.000000  1.000000  \n",
       "house_sim            0.000000  0.000000  0.000000  0.000000  \n",
       "lcs                  5.000000  4.000000  6.000000  9.000000  \n",
       "name_sim             0.476190  0.555556  0.634146  1.000000  \n",
       "phone_missing        1.000000  1.000000  1.000000  1.000000  \n",
       "phone_sim            0.000000  0.000000  0.000000  0.000000  \n",
       "street_name_missing  0.000000  0.000000  0.000000  0.000000  \n",
       "street_name_sim      0.200000  0.090909  0.333333  0.000000  \n",
       "street_type_missing  1.000000  1.000000  1.000000  1.000000  \n",
       "street_type_sim      0.000000  0.000000  0.000000  0.000000  \n",
       "suite_num_missing    1.000000  1.000000  1.000000  1.000000  \n",
       "suite_num_sim        0.000000  0.000000  0.000000  0.000000  \n",
       "suite_type_missing   1.000000  1.000000  1.000000  1.000000  \n",
       "suite_type_sim       0.000000  0.000000  0.000000  0.000000  \n",
       "url_missing          0.000000  0.000000  0.000000  0.000000  \n",
       "url_sim              0.074074  0.153846  1.000000  0.000000  \n",
       "zip_missing          1.000000  1.000000  1.000000  1.000000  \n",
       "zip_sim              0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[wrong_indices, :].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10577</th>\n",
       "      <th>38693</th>\n",
       "      <th>50111</th>\n",
       "      <th>47997</th>\n",
       "      <th>54951</th>\n",
       "      <th>67663</th>\n",
       "      <th>11535</th>\n",
       "      <th>29727</th>\n",
       "      <th>42480</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lc_id</th>\n",
       "      <td>825acefd3e298274a150</td>\n",
       "      <td>edeba23f215dcc702220</td>\n",
       "      <td>e3f9d84c0c989f2e7928</td>\n",
       "      <td>f7bb0b23ce99cddcd5c3</td>\n",
       "      <td>212dffb393f745df801a</td>\n",
       "      <td>493f5e2798de851ec3b2</td>\n",
       "      <td>c170270283ef870d546b</td>\n",
       "      <td>5f3fd107090d0ddc658b</td>\n",
       "      <td>66ef54d76ff989a91d52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_latitude</th>\n",
       "      <td>40.6438</td>\n",
       "      <td>40.7776</td>\n",
       "      <td>40.7746</td>\n",
       "      <td>40.7223</td>\n",
       "      <td>40.7398</td>\n",
       "      <td>40.7582</td>\n",
       "      <td>40.7662</td>\n",
       "      <td>40.714</td>\n",
       "      <td>40.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_longitude</th>\n",
       "      <td>-73.782</td>\n",
       "      <td>-73.9457</td>\n",
       "      <td>-73.9573</td>\n",
       "      <td>-73.988</td>\n",
       "      <td>-73.9896</td>\n",
       "      <td>-73.9923</td>\n",
       "      <td>-73.9778</td>\n",
       "      <td>-73.9969</td>\n",
       "      <td>-73.9785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_name</th>\n",
       "      <td>greenwichvillagebistro</td>\n",
       "      <td>yorkgrill</td>\n",
       "      <td>lukes</td>\n",
       "      <td>karaokebohoorchard</td>\n",
       "      <td>brioflatiron</td>\n",
       "      <td>pickabagel</td>\n",
       "      <td>exhalespa</td>\n",
       "      <td>tsungsunsocialclub</td>\n",
       "      <td>starbucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_phone</th>\n",
       "      <td>7187512890</td>\n",
       "      <td>2127720291</td>\n",
       "      <td>2122497070</td>\n",
       "      <td>2127770102</td>\n",
       "      <td>2126732121</td>\n",
       "      <td>2127928008</td>\n",
       "      <td>2125617400</td>\n",
       "      <td>2122269414</td>\n",
       "      <td>2122658610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_postal_code</th>\n",
       "      <td>11430</td>\n",
       "      <td>10128</td>\n",
       "      <td>10075</td>\n",
       "      <td>10002</td>\n",
       "      <td>10003</td>\n",
       "      <td>10036</td>\n",
       "      <td>10019</td>\n",
       "      <td>10002</td>\n",
       "      <td>10105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_street_address</th>\n",
       "      <td>John F Kennedy International Airport</td>\n",
       "      <td>1690 York Ave</td>\n",
       "      <td>1394 3rd Ave</td>\n",
       "      <td>196 Orchard St</td>\n",
       "      <td>920 Broadway</td>\n",
       "      <td>360 W 42nd St</td>\n",
       "      <td>150 Central Park South</td>\n",
       "      <td>11 Division St</td>\n",
       "      <td>1345 6th Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_website</th>\n",
       "      <td>none</td>\n",
       "      <td>yorkgrillnyc</td>\n",
       "      <td>lukesbarandgrill</td>\n",
       "      <td>karaokeboho</td>\n",
       "      <td>brioflatiron</td>\n",
       "      <td>pickabagel42ndstreetnyc</td>\n",
       "      <td>exhalespa</td>\n",
       "      <td>none</td>\n",
       "      <td>starbucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_house</th>\n",
       "      <td>None</td>\n",
       "      <td>1690</td>\n",
       "      <td>1394</td>\n",
       "      <td>196</td>\n",
       "      <td>920</td>\n",
       "      <td>360</td>\n",
       "      <td>150</td>\n",
       "      <td>11</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_street_name</th>\n",
       "      <td>John F Kennedy International Airport</td>\n",
       "      <td>York</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Orchard</td>\n",
       "      <td>Broadway</td>\n",
       "      <td>W 42nd</td>\n",
       "      <td>Central Park South</td>\n",
       "      <td>Division</td>\n",
       "      <td>6th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_street_type</th>\n",
       "      <td>None</td>\n",
       "      <td>Ave</td>\n",
       "      <td>Ave</td>\n",
       "      <td>St</td>\n",
       "      <td>None</td>\n",
       "      <td>St</td>\n",
       "      <td>None</td>\n",
       "      <td>St</td>\n",
       "      <td>Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_suite_num</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_suite_type</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_id</th>\n",
       "      <td>4f9ab1dbd4f2465542bc673f</td>\n",
       "      <td>51a11cbc498e4083823909f1</td>\n",
       "      <td>51e25e57498e535de72f03e7</td>\n",
       "      <td>51abad92e4b0f65d68fc311d</td>\n",
       "      <td>51e869ac498e7e485cabcdeb</td>\n",
       "      <td>51f119e7498e9716f71f4413</td>\n",
       "      <td>51eb7eed498e401ec51196b6</td>\n",
       "      <td>51ce011a498ed8dfb15381bb</td>\n",
       "      <td>51c9e1dd498e33ecd8670892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_latitude</th>\n",
       "      <td>40.6403</td>\n",
       "      <td>40.7183</td>\n",
       "      <td>40.7201</td>\n",
       "      <td>40.7223</td>\n",
       "      <td>40.7445</td>\n",
       "      <td>40.7564</td>\n",
       "      <td>40.758</td>\n",
       "      <td>40.7726</td>\n",
       "      <td>40.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_longitude</th>\n",
       "      <td>-73.7874</td>\n",
       "      <td>-74.0086</td>\n",
       "      <td>-73.9958</td>\n",
       "      <td>-73.9882</td>\n",
       "      <td>-73.9904</td>\n",
       "      <td>-73.9673</td>\n",
       "      <td>-73.9812</td>\n",
       "      <td>-73.9762</td>\n",
       "      <td>-73.9912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_name</th>\n",
       "      <td>greenwichvillagebar</td>\n",
       "      <td>newyorkdad</td>\n",
       "      <td>maxandmikes</td>\n",
       "      <td>karaokenight</td>\n",
       "      <td>homeplate@flatironschool</td>\n",
       "      <td>freshbagels</td>\n",
       "      <td>halelcart</td>\n",
       "      <td>springsocialrunningclub</td>\n",
       "      <td>starbucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_phone</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_postal_code</th>\n",
       "      <td>11430</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_street_address</th>\n",
       "      <td>John F Kennedy International Airport, Delta Te...</td>\n",
       "      <td>New York</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_website</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_house</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_street_name</th>\n",
       "      <td>John F Kennedy International Airport Delta Ter...</td>\n",
       "      <td>New York</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_street_type</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_suite_num</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_suite_type</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               10577  \\\n",
       "lc_id                                           825acefd3e298274a150   \n",
       "lc_latitude                                                  40.6438   \n",
       "lc_longitude                                                 -73.782   \n",
       "lc_name                                       greenwichvillagebistro   \n",
       "lc_phone                                                  7187512890   \n",
       "lc_postal_code                                                 11430   \n",
       "lc_street_address               John F Kennedy International Airport   \n",
       "lc_website                                                      none   \n",
       "lc_house                                                        None   \n",
       "lc_street_name                  John F Kennedy International Airport   \n",
       "lc_street_type                                                  None   \n",
       "lc_suite_num                                                    None   \n",
       "lc_suite_type                                                   None   \n",
       "fs_id                                       4f9ab1dbd4f2465542bc673f   \n",
       "fs_latitude                                                  40.6403   \n",
       "fs_longitude                                                -73.7874   \n",
       "fs_name                                          greenwichvillagebar   \n",
       "fs_phone                                                        None   \n",
       "fs_postal_code                                                 11430   \n",
       "fs_street_address  John F Kennedy International Airport, Delta Te...   \n",
       "fs_website                                                      none   \n",
       "fs_house                                                        None   \n",
       "fs_street_name     John F Kennedy International Airport Delta Ter...   \n",
       "fs_street_type                                                  None   \n",
       "fs_suite_num                                                    None   \n",
       "fs_suite_type                                                   None   \n",
       "\n",
       "                                      38693                     50111  \\\n",
       "lc_id                  edeba23f215dcc702220      e3f9d84c0c989f2e7928   \n",
       "lc_latitude                         40.7776                   40.7746   \n",
       "lc_longitude                       -73.9457                  -73.9573   \n",
       "lc_name                           yorkgrill                     lukes   \n",
       "lc_phone                         2127720291                2122497070   \n",
       "lc_postal_code                        10128                     10075   \n",
       "lc_street_address             1690 York Ave              1394 3rd Ave   \n",
       "lc_website                     yorkgrillnyc          lukesbarandgrill   \n",
       "lc_house                               1690                      1394   \n",
       "lc_street_name                         York                       3rd   \n",
       "lc_street_type                          Ave                       Ave   \n",
       "lc_suite_num                           None                      None   \n",
       "lc_suite_type                          None                      None   \n",
       "fs_id              51a11cbc498e4083823909f1  51e25e57498e535de72f03e7   \n",
       "fs_latitude                         40.7183                   40.7201   \n",
       "fs_longitude                       -74.0086                  -73.9958   \n",
       "fs_name                          newyorkdad               maxandmikes   \n",
       "fs_phone                               None                      None   \n",
       "fs_postal_code                         None                      None   \n",
       "fs_street_address                  New York                      None   \n",
       "fs_website                             none                      none   \n",
       "fs_house                               None                      None   \n",
       "fs_street_name                     New York                      None   \n",
       "fs_street_type                         None                      None   \n",
       "fs_suite_num                           None                      None   \n",
       "fs_suite_type                          None                      None   \n",
       "\n",
       "                                      47997                     54951  \\\n",
       "lc_id                  f7bb0b23ce99cddcd5c3      212dffb393f745df801a   \n",
       "lc_latitude                         40.7223                   40.7398   \n",
       "lc_longitude                        -73.988                  -73.9896   \n",
       "lc_name                  karaokebohoorchard              brioflatiron   \n",
       "lc_phone                         2127770102                2126732121   \n",
       "lc_postal_code                        10002                     10003   \n",
       "lc_street_address            196 Orchard St              920 Broadway   \n",
       "lc_website                      karaokeboho              brioflatiron   \n",
       "lc_house                                196                       920   \n",
       "lc_street_name                      Orchard                  Broadway   \n",
       "lc_street_type                           St                      None   \n",
       "lc_suite_num                           None                      None   \n",
       "lc_suite_type                          None                      None   \n",
       "fs_id              51abad92e4b0f65d68fc311d  51e869ac498e7e485cabcdeb   \n",
       "fs_latitude                         40.7223                   40.7445   \n",
       "fs_longitude                       -73.9882                  -73.9904   \n",
       "fs_name                        karaokenight  homeplate@flatironschool   \n",
       "fs_phone                               None                      None   \n",
       "fs_postal_code                         None                      None   \n",
       "fs_street_address                      None                      None   \n",
       "fs_website                             none                      none   \n",
       "fs_house                               None                      None   \n",
       "fs_street_name                         None                      None   \n",
       "fs_street_type                         None                      None   \n",
       "fs_suite_num                           None                      None   \n",
       "fs_suite_type                          None                      None   \n",
       "\n",
       "                                      67663                     11535  \\\n",
       "lc_id                  493f5e2798de851ec3b2      c170270283ef870d546b   \n",
       "lc_latitude                         40.7582                   40.7662   \n",
       "lc_longitude                       -73.9923                  -73.9778   \n",
       "lc_name                          pickabagel                 exhalespa   \n",
       "lc_phone                         2127928008                2125617400   \n",
       "lc_postal_code                        10036                     10019   \n",
       "lc_street_address             360 W 42nd St    150 Central Park South   \n",
       "lc_website          pickabagel42ndstreetnyc                 exhalespa   \n",
       "lc_house                                360                       150   \n",
       "lc_street_name                       W 42nd        Central Park South   \n",
       "lc_street_type                           St                      None   \n",
       "lc_suite_num                           None                      None   \n",
       "lc_suite_type                          None                      None   \n",
       "fs_id              51f119e7498e9716f71f4413  51eb7eed498e401ec51196b6   \n",
       "fs_latitude                         40.7564                    40.758   \n",
       "fs_longitude                       -73.9673                  -73.9812   \n",
       "fs_name                         freshbagels                 halelcart   \n",
       "fs_phone                               None                      None   \n",
       "fs_postal_code                         None                      None   \n",
       "fs_street_address                      None                      None   \n",
       "fs_website                             none                      none   \n",
       "fs_house                               None                      None   \n",
       "fs_street_name                         None                      None   \n",
       "fs_street_type                         None                      None   \n",
       "fs_suite_num                           None                      None   \n",
       "fs_suite_type                          None                      None   \n",
       "\n",
       "                                      29727                     42480  \n",
       "lc_id                  5f3fd107090d0ddc658b      66ef54d76ff989a91d52  \n",
       "lc_latitude                          40.714                    40.762  \n",
       "lc_longitude                       -73.9969                  -73.9785  \n",
       "lc_name                  tsungsunsocialclub                 starbucks  \n",
       "lc_phone                         2122269414                2122658610  \n",
       "lc_postal_code                        10002                     10105  \n",
       "lc_street_address            11 Division St              1345 6th Ave  \n",
       "lc_website                             none                 starbucks  \n",
       "lc_house                                 11                      1345  \n",
       "lc_street_name                     Division                       6th  \n",
       "lc_street_type                           St                       Ave  \n",
       "lc_suite_num                           None                      None  \n",
       "lc_suite_type                          None                      None  \n",
       "fs_id              51ce011a498ed8dfb15381bb  51c9e1dd498e33ecd8670892  \n",
       "fs_latitude                         40.7726                     40.73  \n",
       "fs_longitude                       -73.9762                  -73.9912  \n",
       "fs_name             springsocialrunningclub                 starbucks  \n",
       "fs_phone                               None                      None  \n",
       "fs_postal_code                         None                      None  \n",
       "fs_street_address                      None                      None  \n",
       "fs_website                             none                      none  \n",
       "fs_house                               None                      None  \n",
       "fs_street_name                         None                      None  \n",
       "fs_street_type                         None                      None  \n",
       "fs_suite_num                           None                      None  \n",
       "fs_suite_type                          None                      None  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[wrong_indices, :].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction preparation\n",
    "### Model training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=100, max_features=None, n_jobs=-1, random_state=1).fit(train_data, match_column)\n",
    "labels = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with the phone matching set then export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build and export the file\n",
    "lc_col = test['lc_id'][labels2.astype(bool)]\n",
    "fs_col = test['fs_id'][labels2.astype(bool)]\n",
    "\n",
    "for lc_id, fs_id in matches_test.iteritems():\n",
    "    lc_col = lc_col.append(pd.Series(lc_id))\n",
    "    fs_col = fs_col.append(pd.Series(fs_id))\n",
    "\n",
    "output = pd.concat([lc_col, fs_col], axis=1)\n",
    "output.columns = ['locu_id', 'foursquare_id']\n",
    "\n",
    "#with open('20160417.csv', 'w') as f:\n",
    "#    output.to_csv(f, index=False, columns = ['locu_id', 'foursquare_id'])\n",
    "    \n",
    "# Instabase version\n",
    "username = \"diegoll2k\"\n",
    "repo = \"entity-resolution-csds\"\n",
    "with ib.open('/{0}/{1}/fs/Instabase%20Drive/files/matches_4.csv'.format(username,repo)) as f:\n",
    "  output.to_csv(f, index=False, columns = ['locu_id', 'foursquare_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
