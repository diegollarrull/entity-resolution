{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution\n",
    "## Data Loading and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import streetaddress as sa\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "PATH = \"Prakhar/er-assignment/fs/Instabase%20Drive/files/datasets/\"\n",
    "FILES = {\n",
    "    \"foursquare_test\": \"foursquare_test_hard.json\",\n",
    "    \"locu_test\": \"locu_test_hard.json\",\n",
    "    \"matches\": \"matches_train_hard.csv\",\n",
    "    \"foursquare_train\": \"foursquare_train_hard.json\",\n",
    "    \"locu_train\": \"locu_train_hard.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalizes street addresses using the streetaddress library. All normalized fields are added as columns\n",
    "def addr_parse(address):\n",
    "    if address is not None: \n",
    "        addr_parser = sa.StreetAddressParser()\n",
    "        addr = addr_parser.parse(address)\n",
    "        format = {'house': [addr['house']],\n",
    "                  'street_name': [addr['street_name']],\n",
    "                  'street_type': [addr['street_type']],\n",
    "                  'suite_num': [addr['suite_num']],\n",
    "                  'suite_type': [addr['suite_type']] }   \n",
    "    else: \n",
    "        format = {'house': [None],\n",
    "                  'street_name': [None],\n",
    "                  'street_type': [None],\n",
    "                  'suite_num': [None],\n",
    "                  'suite_type': [None] }\n",
    "    rv = pd.DataFrame(data = format)\n",
    "    return rv\n",
    "\n",
    "# fs_train = pd.read_json(ib.open(PATH + FILES[\"foursquare_train\"]))\n",
    "# fs_test = pd.read_json(ib.open(PATH + FILES[\"foursquare_test\"]))\n",
    "# lc_train = pd.read_json(ib.open(PATH + FILES[\"locu_train\"]))\n",
    "# lc_test = pd.read_json(ib.open(PATH + FILES[\"locu_test\"]))\n",
    "# matches = pd.read_csv(ib.open(PATH + FILES[\"matches\"]))\n",
    "fs_train = pd.read_json('data/foursquare_train_hard.json')\n",
    "fs_test = pd.read_json('data/foursquare_test_hard.json')\n",
    "lc_train = pd.read_json('data/locu_train_hard.json')\n",
    "lc_test = pd.read_json('data/locu_test_hard.json')\n",
    "matches = pd.read_csv('data/matches_train_hard.csv')\n",
    "\n",
    "data_list = [fs_train, fs_test, lc_train, lc_test]\n",
    "\n",
    "for df in data_list:\n",
    "    df.drop(['country', 'region', 'locality'], inplace=True, axis=1)\n",
    "    \n",
    "    df.replace([''], [None], inplace=True)\n",
    "    \n",
    "    df['id'] = df['id'].astype('str')\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'])\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'])\n",
    "#     df['locality'] = df['locality'].astype('str')\n",
    "    \n",
    "    # Unicode chars to replace\n",
    "    df['name'].replace([u\"\\xe9\"], ['e'], regex=True, inplace=True)\n",
    "    df['name'].replace([u\"\\xed\"], ['i'], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\u2019'], [''], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\xc7'], ['c'], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\u2013'], ['-'], regex=True, inplace=True)\n",
    "    \n",
    "    df['name'].replace([r':|\\'|,|\\.|-'], [''], regex=True, inplace=True)\n",
    "    df['name'].replace(['&'], ['and'], regex=True, inplace=True)\n",
    "    df['name'].replace(['\\s+|\\/'], [' '], regex=True, inplace=True)\n",
    "    df['name'].replace(['\\s'], [''], regex=True, inplace=True)\n",
    "\n",
    "    df['name'] = df['name'].astype(str).str.lower()\n",
    "    \n",
    "    df['phone'].replace([r'\\(|\\)|\\s|-'], [''], regex=True, inplace=True)\n",
    "    \n",
    "    df['street_address'].replace([r'<sup>|<\\/sup>'], [''], regex=True, inplace=True)\n",
    "    df['street_address'] = df['street_address'].astype(str)\n",
    "    \n",
    "    df['website'].replace([u\"\\u200e\"], [''], regex=True, inplace=True)\n",
    "    df['website'].replace([r'http(s)?://(www.)?|\\\\u200e'], [''], regex=True, inplace=True)\n",
    "    df['website'].replace([r'\\..*'], [''], regex=True, inplace=True)\n",
    "    df['website'] = df['website'].astype(str).str.lower()\n",
    "    df['website'].replace(['None'], [None], inplace=True)\n",
    "    \n",
    "    \n",
    "c = fs_train['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "fs_train = pd.concat([fs_train,cols], axis = 1)\n",
    "\n",
    "c = fs_test['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "fs_test = pd.concat([fs_test,cols], axis = 1)\n",
    "\n",
    "c = lc_train['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "lc_train = pd.concat([lc_train,cols], axis = 1)\n",
    "\n",
    "c = lc_test['street_address'].apply(addr_parse)\n",
    "cols = pd.concat([i for i in c]).reset_index(drop=True)\n",
    "lc_test = pd.concat([lc_test,cols], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append a prefix to identify the columns when concatenated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in [fs_train, fs_test]:\n",
    "    df.columns = ['fs_' + str(i) for i in list(df.columns)]\n",
    "for df in [lc_train, lc_test]:\n",
    "    df.columns = ['lc_' + str(i) for i in list(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Row Combinations\n",
    "LC data is repeated row at a time, then FS data is repeated entirely at a time. The two are concatenated to create the combo data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_left =  lc_train.loc[np.repeat(lc_train.index.values, len(lc_train))].reset_index(drop=True)\n",
    "train_right =  pd.concat([fs_train]*len(fs_train), ignore_index=True)\n",
    "train = pd.concat([train_left, train_right], axis=1)\n",
    "\n",
    "test_left =  lc_test.loc[np.repeat(lc_test.index.values, len(lc_test))].reset_index(drop=True)\n",
    "test_right =  pd.concat([fs_test]*len(fs_test), ignore_index=True)\n",
    "test = pd.concat([test_left, test_right], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_distance(pt1, pt2):\n",
    "    return math.sqrt( (pt1[0] - pt2[0])**2 + (pt1[1] - pt2[1])**2 )\n",
    "    \n",
    "def string_similarity(str1, str2):\n",
    "  return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "def lcs(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in xrange(1 + len(s1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in xrange(1, 1 + len(s1)):\n",
    "        for y in xrange(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    return len(s1[x_longest - longest: x_longest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computed Columns\n",
    "### Match status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Match dictionary\n",
    "match_dict = {}\n",
    "for i, row in matches.iterrows():\n",
    "    match_dict[row['locu_id']] = row['foursquare_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_column = []\n",
    "for i, row in train.iterrows():\n",
    "    lc_id = row['lc_id']\n",
    "    fs_id = row['fs_id']\n",
    "    if (lc_id in match_dict) and (match_dict[lc_id] == fs_id):\n",
    "        match_column.append(1)\n",
    "    else:\n",
    "        match_column.append(0)\n",
    "match_column = np.array(match_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "Starting iteration #0\n",
      "Processing distances...\n",
      "Processing names...\n",
      "Processing ZIP codes...\n",
      "Processing phone numbers...\n",
      "Processing URLs...\n",
      "Processing street addresses...\n",
      "#####\n",
      "Starting iteration #1\n",
      "Processing distances...\n",
      "Processing names...\n",
      "Processing ZIP codes...\n",
      "Processing phone numbers...\n",
      "Processing URLs...\n",
      "Processing street addresses...\n",
      "#####\n",
      "Processing Finished!\n"
     ]
    }
   ],
   "source": [
    "data_list = [train, test]\n",
    "\n",
    "for d_i, df in enumerate(data_list):\n",
    "    print(\"#####\\nStarting iteration #{}\".format(d_i))\n",
    "    \n",
    "    print('Processing distances...')\n",
    "    distance = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_loc = (row['lc_latitude'], row['lc_longitude'])\n",
    "        fs_loc = (row['fs_latitude'], row['fs_longitude'])\n",
    "        distance.append(find_distance(lc_loc, fs_loc))\n",
    "\n",
    "    print('Processing names...')\n",
    "    name_dist = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_name = row['lc_name']\n",
    "        fs_name = row['fs_name']\n",
    "        name_dist.append(string_similarity(lc_name, fs_name))\n",
    "\n",
    "    print('Processing ZIP codes...')\n",
    "    zip_dist = []\n",
    "    zip_missing = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_zip = row['lc_postal_code']\n",
    "        fs_zip = row['fs_postal_code']\n",
    "        if lc_zip and fs_zip:\n",
    "            zip_dist.append(string_similarity(lc_zip, fs_zip))\n",
    "            zip_missing.append(0)\n",
    "        else:\n",
    "            zip_dist.append(np.nan)\n",
    "            zip_missing.append(1)\n",
    "\n",
    "    print('Processing phone numbers...')\n",
    "    phone_dist = []\n",
    "    phone_missing = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_phone = row['lc_phone']\n",
    "        fs_phone = row['fs_phone']\n",
    "        if lc_phone and fs_phone:\n",
    "            phone_dist.append(string_similarity(lc_phone, fs_phone))\n",
    "            phone_missing.append(0)\n",
    "        else:\n",
    "            phone_dist.append(np.nan)\n",
    "            phone_missing.append(1)\n",
    "\n",
    "    print('Processing URLs...')\n",
    "    url_dist = []\n",
    "    url_missing = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_url = row['lc_website']\n",
    "        fs_url = row['fs_website']\n",
    "        if lc_url and fs_url:\n",
    "            url_dist.append(string_similarity(lc_url, fs_url))\n",
    "            url_missing.append(0)\n",
    "        else:\n",
    "            url_dist.append(np.nan)\n",
    "            url_missing.append(1)\n",
    "            \n",
    "    print('Processing street addresses...')\n",
    "    house_sim, house_missing = [], []\n",
    "    street_name_sim, street_name_missing = [], []\n",
    "    street_type_sim, street_type_missing = [], []\n",
    "    suite_num_sim, suite_num_missing = [], []\n",
    "    suite_type_sim, suite_type_missing = [], []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_house = row['lc_house']\n",
    "        fs_house = row['fs_house']\n",
    "        \n",
    "        lc_street_name = row['lc_street_name']\n",
    "        fs_street_name = row['fs_street_name']\n",
    "        \n",
    "        lc_street_type = row['lc_street_type']\n",
    "        fs_street_type = row['fs_street_type']\n",
    "        \n",
    "        lc_suite_num = row['lc_suite_num']\n",
    "        fs_suite_num = row['fs_suite_num']\n",
    "        \n",
    "        lc_suite_type = row['lc_suite_type']\n",
    "        fs_suite_type = row['fs_suite_type']\n",
    "        \n",
    "        if lc_house and fs_house:\n",
    "            house_sim.append(string_similarity(lc_house, fs_house))\n",
    "            house_missing.append(0)\n",
    "        else:\n",
    "            house_sim.append(np.nan)\n",
    "            house_missing.append(1)\n",
    "        \n",
    "        if lc_street_name and fs_street_name:\n",
    "            street_name_sim.append(string_similarity(lc_street_name, fs_street_name))\n",
    "            street_name_missing.append(0)\n",
    "        else:\n",
    "            street_name_sim.append(np.nan)\n",
    "            street_name_missing.append(1)\n",
    "            \n",
    "        if lc_street_type and fs_street_type:\n",
    "            street_type_sim.append(string_similarity(lc_street_type, fs_street_type))\n",
    "            street_type_missing.append(0)\n",
    "        else:\n",
    "            street_type_sim.append(np.nan)\n",
    "            street_type_missing.append(1)\n",
    "        \n",
    "        if lc_suite_num and fs_suite_num:\n",
    "            suite_num_sim.append(string_similarity(lc_suite_num, fs_suite_num))\n",
    "            suite_num_missing.append(0)\n",
    "        else:\n",
    "            suite_num_sim.append(np.nan)\n",
    "            suite_num_missing.append(1)\n",
    "        \n",
    "        if lc_suite_type and fs_suite_type:\n",
    "            suite_type_sim.append(string_similarity(lc_suite_type, fs_suite_type))\n",
    "            suite_type_missing.append(0)\n",
    "        else:\n",
    "            suite_type_sim.append(np.nan)\n",
    "            suite_type_missing.append(1)\n",
    "            \n",
    "    print('Processing LCS...')\n",
    "        lcs_dist = []\n",
    "        lcs_missing = []\n",
    "        for i, row in df.iterrows():\n",
    "            lc_name = row['lc_name']\n",
    "            fs_name = row['fs_name']\n",
    "            if lc_name and fs_name:\n",
    "                lcs_dist.append(lcs(lc_name, fs_name))\n",
    "                lcs_missing.append(0)\n",
    "            else:\n",
    "                lcs_dist.append(np.nan)\n",
    "                lcs_missing.append(1)\n",
    "            \n",
    "    \n",
    "    d = {'distance': distance,\n",
    "         'name_sim': name_dist,\n",
    "         'zip_sim': zip_dist, 'zip_missing': zip_missing,\n",
    "         'phone_sim': phone_dist, 'phone_missing': phone_missing,\n",
    "         'url_sim': url_dist, 'url_missing': url_missing,\n",
    "         'house_sim': house_sim, 'house_missing': house_missing,\n",
    "         'street_name_sim': street_name_sim, 'street_name_missing': street_name_missing,\n",
    "         'street_type_sim': street_type_sim, 'street_type_missing': street_type_missing,\n",
    "         'suite_num_sim': suite_num_sim, 'suite_num_missing': suite_num_missing,\n",
    "         'suite_type_sim': suite_type_sim, 'suite_type_missing': suite_type_missing,\n",
    "         'lcs_sim': lcs_dist, 'lcs_missing': lcs_missing }\n",
    "    \n",
    "    if d_i == 0:\n",
    "        train_data = pd.DataFrame(d)\n",
    "    else:\n",
    "        test_data = pd.DataFrame(d)\n",
    "\n",
    "print(\"#####\\nProcessing Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "ip = Imputer(missing_values = 'NaN')\n",
    "ip.fit(pd.concat([train_data, test_data], axis=0))\n",
    "\n",
    "train_data = pd.DataFrame(ip.fit_transform(train_data))\n",
    "test_data = pd.DataFrame(ip.fit_transform(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* To-do: Output probabilities in order to break ties efficiently\n",
    "* Hyperparameter Tuning\n",
    "\n",
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def cv_run_ada(train_data, train_labels, test_data, test_labels):\n",
    "    model = AdaBoostClassifier(random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_bag(train_data, train_labels, test_data, test_labels):\n",
    "    model = BaggingClassifier(random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_et(train_data, train_labels, test_data, test_labels):\n",
    "    model = ExtraTreesClassifier(n_estimators=50, max_features=None, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_rf(train_data, train_labels, test_data, test_labels):\n",
    "    model = RandomForestClassifier(random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "def cv_run_dt(train_data, train_labels, test_data, test_labels):\n",
    "    model = DecisionTreeClassifier(max_features=None, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972222222222\n",
      "0.972222222222\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.972222222222\n",
      "0.916666666667\n",
      "0.944444444444\n",
      "0.972222222222\n",
      "0.944444444444\n",
      "Overall Recall: 0.969444444444\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(match_column, n_folds=10, random_state=1, shuffle=True)\n",
    "\n",
    "overall_corr = 0\n",
    "for train_index, test_index in skf:\n",
    "    cv_train_data = train_data.loc[train_index]\n",
    "    cv_train_labels = match_column[train_index]\n",
    "    cv_test_data = train_data.loc[test_index]\n",
    "    cv_test_labels = match_column[test_index]\n",
    "    \n",
    "    preds = cv_run_et(cv_train_data, cv_train_labels, cv_test_data, cv_test_labels)\n",
    "\n",
    "    fold_corr = sum(preds[cv_test_labels == 1] == cv_test_labels[cv_test_labels == 1])\n",
    "    overall_corr += fold_corr\n",
    "    \n",
    "    fold_acc = fold_corr / float(sum(cv_test_labels))\n",
    "    print(fold_acc)\n",
    "    \n",
    "print(\"Overall Recall: {}\".format(float(overall_corr) / sum(match_column)))\n",
    "\n",
    "# 10-fold Test Recall\n",
    "# -----\n",
    "# Decision Tree: 0.963888888889\n",
    "# AdaBoost: 0.9583\n",
    "# Random Forest: 0.9639\n",
    "# ExtraTrees (n_estimators=50, max_features=None): 0.963888888889\n",
    "# Logistic: (Fails due to low memory)\n",
    "# Bagging: 0.958\n",
    "# -----\n",
    "# DT: 0.966666666667\n",
    "# ET: 0.969444444444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "## add dumies for missing values in train and test.\n",
    "for column in train_data:\n",
    "    print(train_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## run decision tree with all the data and return probabilities\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def cv_run_dt(train_data, train_labels, test_data, test_labels):\n",
    "    model = DecisionTreeClassifier(max_features=None, random_state=1).fit(train_data, train_labels)\n",
    "    return model.predict(test_data)\n",
    "\n",
    "probs = cv_run_dt(train_data, match_column, test_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run a model with all the data and predict probs and the labels on test\n",
    "# model = AdaBoostClassifier(random_state=1).fit(train_data, match_column)\n",
    "model = DecisionTreeClassifier(random_state=1).fit(train_data, match_column)\n",
    "labels = model.predict(test_data)\n",
    "# probs = model.predict_proba(test_data)[:,1] # not used yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Build and export the file\n",
    "lc_col = test['lc_id'][labels.astype(bool)]\n",
    "fs_col = test['fs_id'][labels.astype(bool)]\n",
    "\n",
    "output = pd.concat([lc_col, fs_col], axis=1)\n",
    "output.columns = ['locu_id', 'foursquare_id']\n",
    "\n",
    "with open('results/20160412-6(DT).csv', 'w') as f:\n",
    "    output.to_csv(f, index=False, columns = ['locu_id', 'foursquare_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample CSV write code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# username = \"woojink\"\n",
    "# repo = \"best-entity-resolvers\"\n",
    "# f = ib.open('/{0}/{1}/fs/Instabase%20Drive/files/matches.csv'.format(username,repo))\n",
    "\n",
    "# # with open('output.csv', 'w') as csvfile:\n",
    "# writer = csv.writer(f)\n",
    "\n",
    "# header = ['locu_id', 'foursquare_id']\n",
    "# writer.writerow(header)\n",
    "# for key, val in matches_pred_test.iteritems():\n",
    "#     writer.writerow([key, val])\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'lc_id', u'lc_latitude', u'lc_longitude', u'lc_name', u'lc_phone',\n",
       "       u'lc_postal_code', u'lc_street_address', u'lc_website', u'lc_house',\n",
       "       u'lc_street_name', u'lc_street_type', u'lc_suite_num', u'lc_suite_type',\n",
       "       u'fs_id', u'fs_latitude', u'fs_longitude', u'fs_name', u'fs_phone',\n",
       "       u'fs_postal_code', u'fs_street_address', u'fs_website', u'fs_house',\n",
       "       u'fs_street_name', u'fs_street_type', u'fs_suite_num',\n",
       "       u'fs_suite_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = None\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
