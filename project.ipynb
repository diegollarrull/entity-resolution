{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution\n",
    "## Data Loading and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "PATH = \"Prakhar/er-assignment/fs/Instabase%20Drive/files/datasets/\"\n",
    "FILES = {\n",
    "    \"foursquare_test\": \"foursquare_test_hard.json\",\n",
    "    \"locu_test\": \"locu_test_hard.json\",\n",
    "    \"matches\": \"matches_train_hard.csv\",\n",
    "    \"foursquare_train\": \"foursquare_train_hard.json\",\n",
    "    \"locu_train\": \"locu_train_hard.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs_train = pd.read_json(ib.open(PATH + FILES[\"foursquare_train\"]))\n",
    "fs_test = pd.read_json(ib.open(PATH + FILES[\"foursquare_test\"]))\n",
    "lc_train = pd.read_json(ib.open(PATH + FILES[\"locu_train\"]))\n",
    "lc_test = pd.read_json(ib.open(PATH + FILES[\"locu_test\"]))\n",
    "matches = pd.read_csv(ib.open(PATH + FILES[\"matches\"]))\n",
    "\n",
    "data_list = [fs_train, fs_test, lc_train, lc_test]\n",
    "\n",
    "for df in data_list:\n",
    "    df.drop(['country', 'region', 'locality'], inplace=True, axis=1)\n",
    "    \n",
    "    df.replace([''], [None], inplace=True)\n",
    "    \n",
    "    df['id'] = df['id'].astype('str')\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'])\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'])\n",
    "#     df['locality'] = df['locality'].astype('str')\n",
    "    \n",
    "    # Unicode chars to replace\n",
    "    df['name'].replace([u\"\\xe9\"], ['e'], regex=True, inplace=True)\n",
    "    df['name'].replace([u\"\\xed\"], ['i'], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\u2019'], [''], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\xc7'], ['c'], regex=True, inplace=True)\n",
    "    df['name'].replace([u'\\u2013'], ['-'], regex=True, inplace=True)\n",
    "    \n",
    "    df['name'].replace([r':|\\'|,|\\.|-'], [''], regex=True, inplace=True)\n",
    "    df['name'].replace(['&'], ['and'], regex=True, inplace=True)\n",
    "    df['name'].replace(['\\s+|\\/'], [' '], regex=True, inplace=True)\n",
    "\n",
    "    df['name'] = df['name'].astype(str).str.lower()\n",
    "    \n",
    "    df['phone'].replace([r'\\(|\\)|\\s|-'], [''], regex=True, inplace=True)\n",
    "    \n",
    "    df['street_address'].replace([r'<sup>|<\\/sup>'], [''], regex=True, inplace=True)\n",
    "    df['street_address'] = df['street_address'].astype(str)\n",
    "    \n",
    "    df['website'].replace([u\"\\u200e\"], [''], regex=True, inplace=True)\n",
    "    df['website'].replace([r'http(s)?://(www.)?|\\\\u200e'], [''], regex=True, inplace=True)\n",
    "    df['website'].replace([r'\\..*'], [''], regex=True, inplace=True)\n",
    "    df['website'] = df['website'].astype(str).str.lower()\n",
    "    df['website'].replace(['None'], [None], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append a prefix to identify the columns when concatenated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in [fs_train, fs_test]:\n",
    "    df.columns = ['fs_' + str(i) for i in list(df.columns)]\n",
    "for df in [lc_train, lc_test]:\n",
    "    df.columns = ['lc_' + str(i) for i in list(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Row Combinations\n",
    "LC data is repeated row at a time, then FS data is repeated entirely at a time. The two are concatenated to create the combo data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_left =  lc_train.loc[np.repeat(lc_train.index.values, len(lc_train))].reset_index(drop=True)\n",
    "train_right =  pd.concat([fs_train]*len(fs_train), ignore_index=True)\n",
    "train = pd.concat([train_left, train_right], axis=1)\n",
    "\n",
    "test_left =  lc_test.loc[np.repeat(lc_test.index.values, len(lc_test))].reset_index(drop=True)\n",
    "test_right =  pd.concat([fs_test]*len(fs_test), ignore_index=True)\n",
    "test = pd.concat([test_left, test_right], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_distance(pt1, pt2):\n",
    "    return math.sqrt( (pt1[0] - pt2[0])**2 + (pt1[1] - pt2[1])**2 )\n",
    "    \n",
    "def string_similarity(str1, str2):\n",
    "  return SequenceMatcher(None, str1, str2).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computed Columns\n",
    "### Match status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Match dictionary\n",
    "match_dict = {}\n",
    "for i, row in matches.iterrows():\n",
    "    match_dict[row['locu_id']] = row['foursquare_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_column = []\n",
    "for i, row in train.iterrows():\n",
    "    lc_id = row['lc_id']\n",
    "    fs_id = row['fs_id']\n",
    "    if (lc_id in match_dict) and (match_dict[lc_id] == fs_id):\n",
    "        match_column.append(1)\n",
    "    else:\n",
    "        match_column.append(0)\n",
    "match_column = np.array(match_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "Starting iteration #0\n",
      "Processing distances...\n",
      "Processing names...\n",
      "Processing ZIP codes...\n",
      "Processing phone numbers...\n",
      "Processing URLs...\n",
      "#####\n",
      "Starting iteration #1\n",
      "Processing distances...\n",
      "Processing names...\n",
      "Processing ZIP codes...\n",
      "Processing phone numbers...\n",
      "Processing URLs...\n"
     ]
    }
   ],
   "source": [
    "data_list = [train, test]\n",
    "\n",
    "for d_i, df in enumerate(data_list):\n",
    "    print(\"#####\\nStarting iteration #{}\".format(d_i))\n",
    "    \n",
    "    print('Processing distances...')\n",
    "    distance = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_loc = (row['lc_latitude'], row['lc_longitude'])\n",
    "        fs_loc = (row['fs_latitude'], row['fs_longitude'])\n",
    "        distance.append(find_distance(lc_loc, fs_loc))\n",
    "\n",
    "    print('Processing names...')\n",
    "    name_dist = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_name = row['lc_name']\n",
    "        fs_name = row['fs_name']\n",
    "        name_dist.append(string_similarity(lc_name, fs_name))\n",
    "\n",
    "    print('Processing ZIP codes...')\n",
    "    zip_dist = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_zip = row['lc_postal_code']\n",
    "        fs_zip = row['fs_postal_code']\n",
    "        if lc_zip and fs_zip:\n",
    "            zip_dist.append(string_similarity(lc_zip, fs_zip))\n",
    "        else:\n",
    "            zip_dist.append(0)\n",
    "\n",
    "    print('Processing phone numbers...')\n",
    "    phone_dist = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_phone = row['lc_phone']\n",
    "        fs_phone = row['fs_phone']\n",
    "        if lc_phone and fs_phone:\n",
    "            phone_dist.append(string_similarity(lc_phone, fs_phone))\n",
    "        else:\n",
    "            phone_dist.append(0)\n",
    "\n",
    "    print('Processing URLs...')\n",
    "    url_dist = []\n",
    "    for i, row in df.iterrows():\n",
    "        lc_url = row['lc_website']\n",
    "        fs_url = row['fs_website']\n",
    "        if lc_url and fs_url:\n",
    "            url_dist.append(string_similarity(lc_url, fs_url))\n",
    "        else:\n",
    "            url_dist.append(0)\n",
    "    \n",
    "    d = {'distance': distance,\n",
    "         'name_dist': name_dist,\n",
    "         'zip_dist': zip_dist,\n",
    "         'phone_dist': phone_dist,\n",
    "         'url_dist': url_dist }\n",
    "    \n",
    "    if d_i == 0:\n",
    "        train_data = pd.DataFrame(d).fillna(0)\n",
    "    else:\n",
    "        test_data = pd.DataFrame(d).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* To-do: Discard all the obviously wrong rows\n",
    "\n",
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "def cv_run_ada(train_data, train_labels, test_data, test_labels):\n",
    "    model = AdaBoostClassifier().fit(train_data, train_labels)\n",
    "    preds = model.predict(test_data)\n",
    "    print(sum(preds))\n",
    "    acc = sum(preds == test_labels)/float(len(test_labels))\n",
    "    return acc\n",
    "\n",
    "def cv_run_rf(train_data, train_labels, test_data, test_labels):\n",
    "    model = RandomForestClassifier(random_state=1).fit(train_data, train_labels)\n",
    "    preds = model.predict(test_data)\n",
    "    print(sum(preds))\n",
    "    acc = sum(preds == test_labels)/float(len(test_labels))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "0.999958333333\n",
      "115\n",
      "0.999958333333\n",
      "113\n",
      "0.999925\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_folds=3, random_state=1, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skf.split(train_data, match_column):\n",
    "    cv_train_data = train_data.loc[train_index]\n",
    "    cv_train_labels = match_column[train_index]\n",
    "    cv_test_data = train_data.loc[test_index]\n",
    "    cv_test_labels = match_column[test_index]\n",
    "    \n",
    "    fold_acc = cv_run_ada(cv_train_data, cv_train_labels, cv_test_data, cv_test_labels)\n",
    "    print(fold_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary testing only using distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'latitude'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c307bcc48b3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfs_train_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc_train_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs_test_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc_test_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfs_train_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlc_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlc_train_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/usernotebook/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/usernotebook/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1802\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'latitude'"
     ]
    }
   ],
   "source": [
    "fs_train_dict, lc_train_dict, fs_test_dict, lc_test_dict = {}, {}, {}, {}\n",
    "for i, row in fs_train.iterrows():\n",
    "    fs_train_dict[row['id']] = (row['latitude'], row['longitude'])\n",
    "for i, row in lc_train.iterrows():\n",
    "    lc_train_dict[row['id']] = (row['latitude'], row['longitude'])\n",
    "for i, row in fs_test.iterrows():\n",
    "    fs_test_dict[row['id']] = (row['latitude'], row['longitude'])\n",
    "for i, row in lc_test.iterrows():\n",
    "    lc_test_dict[row['id']] = (row['latitude'], row['longitude'])\n",
    "    \n",
    "matches_pred = {}\n",
    "for lc_key, lc_loc in lc_train_dict.iteritems():\n",
    "    min_dist = float(\"inf\")\n",
    "    min_loc = None\n",
    "    for fs_key, fs_loc in fs_train_dict.iteritems():\n",
    "        dist = find_distance(lc_loc, fs_loc)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_key = fs_key\n",
    "    matches_pred[lc_key] = min_key\n",
    "    \n",
    "count = 0\n",
    "for _, row in matches.iterrows():\n",
    "    lc_id = row['locu_id']\n",
    "    fs_id = row['foursquare_id']\n",
    "    if matches_pred[lc_id] == fs_id:\n",
    "        count += 1\n",
    "print(count / float(len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_pred_test = {}\n",
    "for lc_key, lc_loc in lc_test_dict.iteritems():\n",
    "    min_dist = float(\"inf\")\n",
    "    min_loc = None\n",
    "    for fs_key, fs_loc in fs_test_dict.iteritems():\n",
    "        dist = find_distance(lc_loc, fs_loc)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_key = fs_key\n",
    "    matches_pred_test[lc_key] = min_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "username = \"woojink\"\n",
    "repo = \"best-entity-resolvers\"\n",
    "f = ib.open('/{0}/{1}/fs/Instabase%20Drive/files/matches.csv'.format(username,repo))\n",
    "\n",
    "# with open('output.csv', 'w') as csvfile:\n",
    "writer = csv.writer(f)\n",
    "\n",
    "header = ['locu_id', 'foursquare_id']\n",
    "writer.writerow(header)\n",
    "for key, val in matches_pred_test.iteritems():\n",
    "    writer.writerow([key, val])\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
